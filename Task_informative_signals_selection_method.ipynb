{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import APIs\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import resample\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Control GPU consumption.\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62, 3000, 100) (62, 3000, 100)\n",
      "(2, 100) (2, 100)\n",
      "(100, 20, 250, 1) (100, 20, 250, 1)\n",
      "(100, 2) (100, 2)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "# I exploited Lee et al.'s, GigaScience, 2019.\n",
    "# The entire dataset is available at \"https://academic.oup.com/gigascience/article/8/5/giz002/5304369.\"\n",
    "# path = "Define/Your/Own/Path"\n",
    "train_X, train_Y = np.load(path + \"train.npy\"), np.load(path + \"trlbl.npy\")\n",
    "test_X, test_Y = np.load(path + \"test.npy\"), np.load(path + \"tslbl.npy\")\n",
    "\n",
    "# To check the size of data and label (C: # electrodes, T: # timepoints).\n",
    "print(train_X.shape, test_X.shape) # (C, T, # trials)\n",
    "print(train_Y.shape, test_Y.shape) # (# classes, # trials)\n",
    "\n",
    "# Select electrode channels on the sensorymotor area.\n",
    "ch_list = [7, 32, 8, 9, 33, 10, 34, 12, 35, 13, 36, 14, 37, 17, 38, 18, 39, 19, 40, 20]\n",
    "# We already conducted BPF (8~30Hz) and segments (0.5~3.5 sec).\n",
    "# Segments from 1 to 3.5-second (We used preprocessed data).\n",
    "train_X, test_X = train_X[ch_list, 500:, :], test_X[ch_list, 500:, :]\n",
    "# Downsample to 100Hz sampling rate.\n",
    "train_X = resample(train_X, int(train_X.shape[1] * 0.1), axis=1)\n",
    "test_X = resample(test_X, int(test_X.shape[1] * 0.1), axis=1)\n",
    "\n",
    "# Reshape the data.\n",
    "train_X = np.expand_dims(np.moveaxis(train_X, -1, 0), -1)\n",
    "test_X = np.expand_dims(np.moveaxis(test_X, -1, 0), -1)\n",
    "train_Y, test_Y = np.swapaxes(train_Y, 0, 1), np.swapaxes(test_Y, 0, 1)\n",
    "# To check the size.\n",
    "print(train_X.shape, test_X.shape) # (# trials, C, T, 1)\n",
    "print(train_Y.shape, test_Y.shape) # (# trials, # classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a deep neural network-based BCI model.\n",
    "# Here, we used Ko et al.'s, arXiv, 2020 (MSNN).\n",
    "# For detail, see \"https://arxiv.org/abs/2003.02657.\"\n",
    "class MSNN(tf.keras.Model):\n",
    "    tf.keras.backend.set_floatx(\"float64\")\n",
    "    def __init__(self):\n",
    "        super(MSNN, self).__init__()\n",
    "        self.C = 20\n",
    "        self.fs = 100\n",
    "\n",
    "        # Regularizer\n",
    "        self.regularizer = tf.keras.regularizers.L1L2(l1=.001, l2=.01)\n",
    "\n",
    "        # Activation functions\n",
    "        self.activation = tf.keras.layers.LeakyReLU()\n",
    "        self.softmax = tf.keras.layers.Softmax()\n",
    "        \n",
    "        # Define convolutions\n",
    "        conv = lambda D, kernel : tf.keras.layers.Conv2D(D, kernel, kernel_regularizer=self.regularizer)\n",
    "        sepconv = lambda D, kernel : tf.keras.layers.SeparableConv2D(D, kernel, padding=\"same\",\n",
    "                                                                    depthwise_regularizer=self.regularizer,\n",
    "                                                                    pointwise_regularizer=self.regularizer)\n",
    "        \n",
    "        # Spectral convoltuion\n",
    "        self.conv0 = conv(4, (1, int(self.fs/2)))\n",
    "        \n",
    "        # Spatio-temporal convolution\n",
    "        self.conv1t = sepconv(16, (1, 25))\n",
    "        self.conv1s = conv(16, (self.C, 1))\n",
    "        \n",
    "        self.conv2t = sepconv(32, (1, 15))\n",
    "        self.conv2s = conv(32, (self.C, 1))\n",
    "        \n",
    "        self.conv3t = sepconv(64, (1, 6))\n",
    "        self.conv3s = conv(64, (self.C, 1))\n",
    "\n",
    "        # Flatteninig\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
    "\n",
    "        # Decision making\n",
    "        self.dense = tf.keras.layers.Dense(2, activation=None, kernel_regularizer=self.regularizer)\n",
    "\n",
    "    def embedding(self, x, random_mask=False):\n",
    "        x = self.activation(self.conv0(x))\n",
    "\n",
    "        x = self.activation(self.conv1t(x))\n",
    "        f1 = self.activation(self.conv1s(x))\n",
    "\n",
    "        x = self.activation(self.conv2t(x))\n",
    "        f2 = self.activation(self.conv2s(x))\n",
    "\n",
    "        x = self.activation(self.conv3t(x))\n",
    "        f3 = self.activation(self.conv3s(x))\n",
    "\n",
    "        feature = tf.concat((f1, f2, f3), -1)\n",
    "        return feature\n",
    "\n",
    "    def classifier(self, feature):\n",
    "        # Flattening, dropout, mapping into the decision nodes\n",
    "        feature = self.flatten(feature)\n",
    "        feature = self.dropout(feature)\n",
    "        y_hat = self.softmax(self.dense(feature))\n",
    "        return y_hat\n",
    "\n",
    "    def GAP(self, feature):\n",
    "        return tf.reduce_mean(feature, -2)\n",
    "\n",
    "    def call(self, x):\n",
    "        # Extract feature using MSNN encoder\n",
    "        feature = self.embedding(x)\n",
    "\n",
    "        # Global Average Pooling\n",
    "        feature = self.GAP(feature)\n",
    "\n",
    "        # Decision making\n",
    "        y_hat = self.classifier(feature)\n",
    "        return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the informative segments selection agent module.\n",
    "# Define actor network (for categorical actions: selection/rejection)\n",
    "class ACTOR(tf.keras.Model):\n",
    "    def __init__(self, n_actions=2):\n",
    "        super().__init__()\n",
    "        self.actor = tf.keras.layers.Dense(n_actions, activation=None, \n",
    "                                          kernel_regularizer=tf.keras.regularizers.L1L2(l1=.001, l2=.01))        \n",
    "    def call(self, segment):\n",
    "        return self.actor(segment) # Outputs logit vector.\n",
    "    \n",
    "# Define critic network\n",
    "class CRITIC(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.critic = tf.keras.layers.Dense(1, activation=None,\n",
    "                                           kernel_regularizer=tf.keras.regularizers.L1L2(l1=.001, l2=.01))\n",
    "    def call(self, segment):\n",
    "        return tf.keras.activations.sigmoid(self.critic(segment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define utility functions.\n",
    "def gradient(model, inputs, labels, mask=None):\n",
    "    with tf.GradientTape() as tape:\n",
    "        if mask is None:\n",
    "            yhat = model(inputs)\n",
    "        else:\n",
    "            feature = model.GAP(model.embedding(inputs) * mask)\n",
    "            yhat = model.classifier(feature)\n",
    "\n",
    "        loss = tf.keras.losses.binary_crossentropy(labels, yhat)\n",
    "\n",
    "    grad = tape.gradient(loss, model.trainable_variables)\n",
    "    return loss, grad\n",
    "\n",
    "def agent_gradient(model, actor, critic, inputs, feature, labels, state, state_next):\n",
    "    gamma = 0.95 # discount factor\n",
    "    with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
    "        loss_FM = tf.keras.losses.binary_crossentropy(labels, model(inputs))\n",
    "        loss_AM = tf.keras.losses.binary_crossentropy(labels, model.classifier(feature))\n",
    "\n",
    "        # Reward, r_t\n",
    "        reward = loss_FM - loss_AM\n",
    "        # Advantage, A_t\n",
    "        advantage = reward[:, None] + gamma * critic(state_next) - critic(state)            \n",
    "        # Critic loss\n",
    "        critic_loss = 0.5 * tf.math.square(advantage)            \n",
    "        # Actor loss\n",
    "        actor_loss = -tf.math.log(tf.nn.softmax(actor(state))) * advantage\n",
    "\n",
    "    critic_grad = tape1.gradient(critic_loss, critic.trainable_variables)\n",
    "    actor_grad = tape2.gradient(actor_loss, actor.trainable_variables)\n",
    "    return critic_loss, critic_grad, actor_loss, actor_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START TRAINING Subject 1, Session 2\n",
      "Iteration 1, Training Loss 0.7189\n",
      "Iteration 2, Training Loss 0.6930\n",
      "Iteration 3, Training Loss 0.6827\n",
      "Iteration 4, Training Loss 0.6638\n",
      "Iteration 5, Training Loss 0.6272\n",
      "Iteration 6, Training Loss 0.5737\n",
      "Iteration 7, Training Loss 0.5169\n",
      "Iteration 8, Training Loss 0.4699\n",
      "Iteration 9, Training Loss 0.4347\n",
      "Iteration 10, Training Loss 0.4076\n",
      "Iteration 11, Training Loss 0.3856\n",
      "Iteration 12, Training Loss 0.3665\n",
      "Iteration 13, Training Loss 0.3483\n",
      "Iteration 14, Training Loss 0.3296\n",
      "Iteration 15, Training Loss 0.3095\n",
      "Iteration 16, Training Loss 0.2831\n",
      "Iteration 17, Training Loss 0.3357\n",
      "Iteration 18, Training Loss 0.2467\n",
      "Iteration 19, Training Loss 0.2275\n",
      "Iteration 20, Training Loss 0.2807\n",
      "Iteration 21, Training Loss 0.1973\n",
      "Iteration 22, Training Loss 0.2404\n",
      "Iteration 23, Training Loss 0.2224\n",
      "Iteration 24, Training Loss 0.2568\n",
      "Iteration 25, Training Loss 0.1640\n",
      "Iteration 26, Training Loss 0.1560\n",
      "Iteration 27, Training Loss 0.1689\n",
      "Iteration 28, Training Loss 0.1638\n",
      "Iteration 29, Training Loss 0.2107\n",
      "Iteration 30, Training Loss 0.1293\n",
      "\n",
      "Subject 1, Session 2,        Testing accuracy: 0.83!\n",
      "\n",
      "Iteration 1, Training Loss 0.7243\n",
      "Iteration 2, Training Loss 0.6993\n",
      "Iteration 3, Training Loss 0.6908\n",
      "Iteration 4, Training Loss 0.6824\n",
      "Iteration 5, Training Loss 0.6682\n",
      "Iteration 6, Training Loss 0.6414\n",
      "Iteration 7, Training Loss 0.5986\n",
      "Iteration 8, Training Loss 0.5463\n",
      "Iteration 9, Training Loss 0.4974\n",
      "Iteration 10, Training Loss 0.4569\n",
      "Iteration 11, Training Loss 0.4261\n",
      "Iteration 12, Training Loss 0.4176\n",
      "Iteration 13, Training Loss 0.3837\n",
      "Iteration 14, Training Loss 0.3660\n",
      "Iteration 15, Training Loss 0.3500\n",
      "Iteration 16, Training Loss 0.3343\n",
      "Iteration 17, Training Loss 0.3180\n",
      "Iteration 18, Training Loss 0.3012\n",
      "Iteration 19, Training Loss 0.2845\n",
      "Iteration 20, Training Loss 0.2567\n",
      "Iteration 21, Training Loss 0.2341\n",
      "Iteration 22, Training Loss 0.2008\n",
      "Iteration 23, Training Loss 0.1859\n",
      "Iteration 24, Training Loss 0.1765\n",
      "Iteration 25, Training Loss 0.1532\n",
      "Iteration 26, Training Loss 0.1692\n",
      "Iteration 27, Training Loss 0.1531\n",
      "Iteration 28, Training Loss 0.1311\n",
      "Iteration 29, Training Loss 0.1090\n",
      "Iteration 30, Training Loss 0.0892\n",
      "\n",
      "Subject 1, Session 2,         Testing accuracy: 0.85!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define experiment conducting class.\n",
    "# Here, we trained and tested MSNN without the proposed agent module.\n",
    "class experiment():\n",
    "    def __init__(self, train_X, train_Y, test_X, test_Y):\n",
    "        # Load dataset.\n",
    "        # For simplicity, we just removed validating phase here.\n",
    "        self.Xtr, self.Ytr = train_X, train_Y\n",
    "        self.Xts, self.Yts = test_X, test_Y\n",
    "        self.Yts = np.argmax(self.Yts, axis=-1) # To use scikit-learn accuracy function\n",
    "        \n",
    "        # Randomize the training dataset.\n",
    "        rand_idx = np.random.permutation(self.Xtr.shape[0])\n",
    "        self.Xtr, self.Ytr = self.Xtr[rand_idx, :, :, :], self.Ytr[rand_idx, :]\n",
    "\n",
    "        # Learning schedules\n",
    "        self.init_LR = 1e-3\n",
    "        self.num_epochs_pre = 10 # Pre-training epochs\n",
    "        self.num_epochs = 30\n",
    "        self.num_batch = 20\n",
    "        self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=self.init_LR)\n",
    "        \n",
    "        # Here, we used subject 1's 2nd session data.\n",
    "        self.sbj_idx, self.sess_idx = 1, 2\n",
    "        print(f\"START TRAINING Subject {self.sbj_idx}, Session {self.sess_idx}\")\n",
    "        \n",
    "        # Call optimizer.\n",
    "        self.num_batch_iter = int(self.Xtr.shape[0]/self.num_batch)\n",
    "        \n",
    "    def training_FM(self):\n",
    "        # Call MSNN.\n",
    "        msnn = MSNN()\n",
    "        \n",
    "        # To record the loss curve.\n",
    "        loss_FM = []\n",
    "        for epoch in range(self.num_epochs):\n",
    "            loss_per_epoch = 0\n",
    "\n",
    "            for batch in range(self.num_batch_iter):\n",
    "                # Sample minibatch.\n",
    "                xb = self.Xtr[batch * self.num_batch : (batch + 1) * self.num_batch, :, :, :]\n",
    "                yb = self.Ytr[batch * self.num_batch : (batch + 1) * self.num_batch, :]\n",
    "\n",
    "                # Estimate loss\n",
    "                loss, grads = gradient(msnn, xb, yb)\n",
    "\n",
    "                # Update the parameters\n",
    "                self.optimizer.apply_gradients(zip(grads, msnn.trainable_variables))\n",
    "                loss_FM.append(np.mean(loss))\n",
    "                loss_per_epoch += np.mean(loss)\n",
    "\n",
    "            loss_per_epoch /= self.num_batch_iter\n",
    "\n",
    "            # Reporting\n",
    "            print(f\"Iteration {epoch + 1}, Training Loss {loss_per_epoch:>.04f}\")\n",
    "            \n",
    "        # Test the learned model.\n",
    "        Yts_hat = np.argmax(msnn(test_X), axis=-1)\n",
    "        print(f\"\\nSubject {self.sbj_idx}, Session {self.sess_idx},\\\n",
    "        Testing accuracy: {accuracy_score(self.Yts, Yts_hat)}!\\n\")\n",
    "        return loss_FM\n",
    "    \n",
    "    def training_AM(self):\n",
    "        # Call MSNN.\n",
    "        msnn = MSNN()\n",
    "        \n",
    "        # To record the loss curve.\n",
    "        loss_AM = []\n",
    "        # Pre-training without the agent module\n",
    "        for epoch in range(self.num_epochs_pre):\n",
    "            loss_per_epoch = 0\n",
    "\n",
    "            for batch in range(self.num_batch_iter):\n",
    "                # Sample minibatch.\n",
    "                xb = self.Xtr[batch * self.num_batch : (batch + 1) * self.num_batch, :, :, :]\n",
    "                yb = self.Ytr[batch * self.num_batch : (batch + 1) * self.num_batch, :]\n",
    "\n",
    "                # Estimate loss\n",
    "                loss, grads = gradient(msnn, xb, yb)\n",
    "\n",
    "                # Update the parameters\n",
    "                self.optimizer.apply_gradients(zip(grads, msnn.trainable_variables))\n",
    "                loss_AM.append(np.mean(loss))\n",
    "                loss_per_epoch += np.mean(loss)\n",
    "\n",
    "            loss_per_epoch /= self.num_batch_iter\n",
    "\n",
    "            # Reporting\n",
    "            print(f\"Iteration {epoch + 1}, Training Loss {loss_per_epoch:>.04f}\")\n",
    "            \n",
    "        # Call agent module.\n",
    "        actor = ACTOR()\n",
    "        critic = CRITIC()\n",
    "        \n",
    "        # Training with the agent module\n",
    "        for epoch in range(self.num_epochs - self.num_epochs_pre):\n",
    "            loss_per_epoch = 0\n",
    "            \n",
    "            for batch in range(self.num_batch_iter):\n",
    "                # Sample minibatch.\n",
    "                xb = self.Xtr[batch * self.num_batch : (batch + 1) * self.num_batch, :, :, :]\n",
    "                yb = self.Ytr[batch * self.num_batch : (batch + 1) * self.num_batch, :]\n",
    "\n",
    "                # Extract full segments.\n",
    "                features = msnn.embedding(xb)\n",
    "                \n",
    "                agg_wo_current = np.zeros((self.num_batch, features.shape[-1]))\n",
    "                num_added = np.zeros((self.num_batch, features.shape[-1])) # To estimate the denominator.\n",
    "                mask = np.zeros(features.shape) # Mask generated by the agent module\n",
    "                for t in range(features.shape[-2] - 1): # t = 1,...,T'\n",
    "\n",
    "                    deno1 = np.copy(num_added)\n",
    "                    deno2 = np.copy(num_added) + 1 # For the features with the current segment.\n",
    "                    # To avoid zero-division.\n",
    "                    deno1[deno1 == 0] = 1.\n",
    "                    \n",
    "                    agg_w_current = agg_wo_current + features[:, 0, t, :]\n",
    "                    \n",
    "                    # Define state, s_t.\n",
    "                    state = np.concatenate((agg_wo_current/deno1, agg_w_current/deno2), axis=-1)\n",
    "                    # Get action, a_t.\n",
    "                    action_probs = actor(state)\n",
    "                    action = np.tile(tf.random.categorical(action_probs, 1).numpy(), 112) # (5, 112)\n",
    "                    mask[:, 0, t, :] = action\n",
    "                    num_added += action\n",
    "                    \n",
    "                    # Current feature after action decision, phi_t.\n",
    "                    deno3 = np.copy(num_added)\n",
    "                    deno3[deno3 == 0] = 1 # To avoid zero-division.\n",
    "                    feature = (agg_wo_current + features[:, 0, t, :] * action)/deno3\n",
    "                    \n",
    "                    # Define next state, s_{t+1}, temporally.\n",
    "                    agg_wo_current = feature\n",
    "                    tmp = agg_wo_current + features[:, 0, t + 1, :]\n",
    "                    state_next = np.concatenate((agg_wo_current/deno3, tmp/(deno3 + 1)), axis=-1)\n",
    "\n",
    "                    # Calculate critic and actor loss values\n",
    "                    critic_loss, critic_grads, actor_loss, actor_grads =\\\n",
    "                    agent_gradient(msnn, actor, critic, xb, feature, yb, state, state_next)\n",
    "                    \n",
    "                    self.optimizer.apply_gradients(zip(critic_grads, critic.trainable_variables))\n",
    "                    self.optimizer.apply_gradients(zip(actor_grads, actor.trainable_variables))\n",
    "                                        \n",
    "                # Finally, predict labels of input EEG using the selected segments.                \n",
    "                # Update the parameters\n",
    "                loss, grads = gradient(msnn, xb, yb, mask)\n",
    "                self.optimizer.apply_gradients(zip(grads, msnn.trainable_variables))\n",
    "                loss_AM.append(np.mean(loss))\n",
    "                loss_per_epoch += np.mean(loss)\n",
    "                \n",
    "            loss_per_epoch /= self.num_batch_iter\n",
    "\n",
    "            # Reporting\n",
    "            print(f\"Iteration {epoch + 1 + self.num_epochs_pre}, Training Loss {loss_per_epoch:>.04f}\")\n",
    "        \n",
    "        # Test the learned model.\n",
    "        Yts_hat = np.argmax(msnn(test_X), axis=-1)\n",
    "        print(f\"\\nSubject {self.sbj_idx}, Session {self.sess_idx}, \\\n",
    "        Testing accuracy: {accuracy_score(self.Yts, Yts_hat)}!\\n\")\n",
    "        return loss_AM\n",
    "        \n",
    "exp = experiment(train_X, train_Y, test_X, test_Y)\n",
    "loss_FM = exp.training_FM()\n",
    "loss_AM = exp.training_AM()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
